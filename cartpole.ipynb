{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teaching A Deep Q Neural Network How To Balance\n",
    "![Cat Balance](https://i.imgur.com/YUEtPEu.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from mish import Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Seed\n",
    "seed = 123\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameterss\n",
    "discount_factor = 0.99\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "batch_size = 32\n",
    "train_start = 1000\n",
    "memory_size = 10000\n",
    "n_episodes = 1000\n",
    "n_win_ticks = 195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(96, input_dim=state_size, kernel_initializer='he_uniform'))\n",
    "    model.add(Mish())\n",
    "    model.add(Dense(48, kernel_initializer='he_uniform'))\n",
    "    model.add(Mish())\n",
    "    model.add(Dense(24, kernel_initializer='he_uniform'))\n",
    "    model.add(Mish())\n",
    "    model.add(Dense(action_size, kernel_initializer='he_uniform'))\n",
    "    model.compile(Adam(), loss='mse')\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "target_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "# Source: https://github.com/yanpanlau/CartPole/blob/master/DQN/CartPole_DQN.py\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "def get_action(state):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    else:\n",
    "        q_value = model.predict(state)\n",
    "        return np.argmax(q_value[0])\n",
    "\n",
    "def train_replay():\n",
    "    if len(memory) < train_start:\n",
    "        return\n",
    "    minibatch = random.sample(memory,  min(batch_size, len(memory)))\n",
    "    state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
    "    state_t = np.concatenate(state_t)\n",
    "    state_t1 = np.concatenate(state_t1)\n",
    "    targets = model.predict(state_t)\n",
    "    Q_sa = target_model.predict(state_t1)\n",
    "    targets[range(batch_size), action_t] = reward_t + discount_factor*np.max(Q_sa, axis=1)*np.invert(terminal)\n",
    "    model.train_on_batch(state_t, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] Score: 27.0\n",
      "[Episode 100] Score: 169.0\n",
      "[Episode 33] Solved! \\o/\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "scores = deque(maxlen=100)\n",
    "episodes = []\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "def learn_to_balance():\n",
    "    epsilon = 1.0 # Start with randomness\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            action = get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            memory.append((state, action, reward if not done else -100, next_state, done))\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay # Decrease randomness\n",
    "            train_replay()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            \n",
    "            env.render()\n",
    "\n",
    "            if done:\n",
    "                env.reset()\n",
    "                update_target_model()\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "\n",
    "                if e % 100 == 0:\n",
    "                    print('[Episode {}] Score: {}'.format(e, score))\n",
    "\n",
    "                if np.mean(scores) >= n_win_ticks:\n",
    "                    print('[Episode {}] Solved! \\o/'.format(e - 100))\n",
    "                    return\n",
    "learn_to_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
